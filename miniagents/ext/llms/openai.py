"""
This module integrates OpenAI language models with MiniAgents.
"""

import typing
from functools import cache
from typing import Any, Optional, Union

from pydantic import Field, field_validator

from miniagents.ext.agents.history_agents import markdown_llm_logger_agent
from miniagents.ext.llms.llm_utils import AssistantMessage, LLMAgent, PromptLogMessage
from miniagents.messages import MessageTokenAppender, Message
from miniagents.miniagents import miniagent, InteractionContext, MiniAgent, MiniAgents
from miniagents.promising.ext.frozen import Frozen

if typing.TYPE_CHECKING:
    import openai as openai_original


class OpenAIMessage(AssistantMessage):
    """
    A message generated by an OpenAI model.
    """


class EmbeddingMessage(Message):
    """
    A message that contains an embedding (a vector of numbers).
    """

    embedding: tuple[float, ...]


@cache
def _default_openai_client() -> "openai_original.AsyncOpenAI":
    try:
        # pylint: disable=import-outside-toplevel
        # noinspection PyShadowingNames
        import openai as openai_original
    except ModuleNotFoundError as exc:
        raise ImportError(
            "The 'openai' package is required for the 'openai' extension of MiniAgents. "
            "Please install it via 'pip install -U openai'."
        ) from exc

    return openai_original.AsyncOpenAI()


@miniagent
class OpenAIAgent(LLMAgent):
    """
    An agent that represents Large Language Models by OpenAI. Check out the implementation of the async `__call__`
    method in the base class `LLMAgent` to understand how agents like this one work (the two most important methods
    of all class-based miniagents are `__init__` and `__call__`).
    TODO Oleksandr: explain parameters
    """

    n: int = 1
    async_client: Any = Field(default_factory=_default_openai_client)
    response_message_class: type[Message] = OpenAIMessage

    # noinspection PyNestedDecorators
    @field_validator("n")
    @classmethod
    def _validate_n(cls, n: int) -> int:
        # TODO Oleksandr: stop complaining about n, support batch mode instead
        if n != 1:
            raise ValueError("Only n=1 is supported by MiniAgents for AsyncOpenAI().chat.completions.create()")
        return n

    async def _produce_tokens(self, message_dicts: list[dict[str, Any]], token_appender: MessageTokenAppender) -> None:
        """
        TODO Oleksandr: docstring
        """
        openai_response = await self.async_client.chat.completions.create(
            messages=message_dicts, model=self.model, stream=self.stream, n=self.n, **self.__pydantic_extra__
        )
        if self.stream:
            async for chunk in openai_response:
                if len(chunk.choices) != 1:  # TODO Oleksandr: do I really need to check it for every token ?
                    raise RuntimeError(
                        f"exactly one Choice was expected from OpenAI, "
                        f"but {len(openai_response.choices)} were returned instead"
                    )
                token = chunk.choices[0].delta.content
                if token:
                    token_appender.append(token)

                token_appender.metadata_so_far["role"] = (
                    chunk.choices[0].delta.role or token_appender.metadata_so_far["role"]
                )
                self._merge_openai_dicts(
                    token_appender.metadata_so_far,
                    chunk.model_dump(exclude={"choices": {0: {"index": ..., "delta": {"content": ..., "role": ...}}}}),
                )
        else:
            if len(openai_response.choices) != 1:
                raise RuntimeError(
                    f"exactly one Choice was expected from OpenAI, "
                    f"but {len(openai_response.choices)} were returned instead"
                )
            # send the complete message content as a single token
            token_appender.append(openai_response.choices[0].message.content)

            token_appender.metadata_so_far["role"] = openai_response.choices[0].message.role
            token_appender.metadata_so_far.update(
                openai_response.model_dump(
                    exclude={"choices": {0: {"index": ..., "message": {"content": ..., "role": ...}}}}
                )
            )

    async def _prepare_message_dicts(self) -> list[dict[str, Any]]:
        """
        TODO Oleksandr: docstring
        """
        if self.system is None:
            message_dicts = []
        else:
            message_dicts = [
                {
                    "role": "system",
                    "content": self.system,
                },
            ]
        message_dicts.extend(self._message_to_llm_dict(msg) for msg in await self.ctx.message_promises)
        return message_dicts

    @classmethod
    def _merge_openai_dicts(cls, destination_dict: dict[str, Any], dict_to_merge: dict[str, Any]) -> None:
        """
        Merge the dict_to_merge into the destination_dict.
        """
        for key, value in dict_to_merge.items():
            if value is not None:
                existing_value = destination_dict.get(key)
                if isinstance(existing_value, dict):
                    cls._merge_openai_dicts(existing_value, value)
                elif isinstance(existing_value, list):
                    if key == "choices":
                        if not existing_value:
                            destination_dict[key] = [{}]  # we only expect a single choice in our implementation
                        cls._merge_openai_dicts(destination_dict[key][0], value[0])
                    else:
                        destination_dict[key].extend(value)
                else:
                    destination_dict[key] = value


@miniagent
async def openai_embedding_agent(
    ctx: InteractionContext,
    model: str,
    async_client: Any = None,
    batch_mode: bool = False,  # TODO Oleksandr: which default value is better ? Put it into MiniAgents ?
    response_metadata: Optional[Frozen] = None,
    llm_logger_agent: Optional[Union[MiniAgent, bool]] = None,
    **kwargs,
) -> None:
    """
    An agent that produces embedding(s) for text(s) of the provided message(s) using OpenAI embedding models.
    TODO Oleksandr: explain parameters (especially `batch_mode`)
    """
    if not async_client:
        async_client = _default_openai_client()

    if llm_logger_agent is None:
        llm_logger_agent = MiniAgents.get_current().llm_logger_agent
    if llm_logger_agent is True:
        llm_logger_agent = markdown_llm_logger_agent

    if batch_mode:
        # we are in batch mode - let's collect all the messages and produce embeddings for each of them
        texts = [str(await msg_promise) async for msg_promise in ctx.message_promises]
    else:
        # if we are not in batch mode, and we still receive multiple messages, we will concatenate them all into
        # a single piece of text with parts (messages) separated by double newlines
        texts = [str(await ctx.message_promises.as_single_promise())]

    data = (await async_client.embeddings.create(input=texts, model=model, **kwargs)).data

    response_metadata_dict = dict(response_metadata or Frozen())

    embedding_messages = [EmbeddingMessage(embedding=entry.embedding, **response_metadata_dict) for entry in data]
    ctx.reply(embedding_messages)

    if llm_logger_agent:
        llm_logger_agent.kick_off(
            list(zip([PromptLogMessage(content=text, role="user") for text in texts], embedding_messages)),
            request_metadata={
                "agent_alias": ctx.this_agent.alias,
                "model": model,
                "batch_mode": batch_mode,
                **kwargs,
            },
            show_response_metadata=False,
        )
